# -*- coding: utf-8 -*-
"""
diyapi_anti_entropy_server.py

Performs weekly or monthly consistency checks on every avatar.
Query each machine for a "database consistency check hash" 
(see below) for the avatar.
Every machine on the network replies with it's consistency check hash for 
that avatar.
If consistency hashes match, done, move on to next avatar.
If consistency hashes don't match, schedule avatar for recheck in an hour.
For any avatar that misses 3 consistency checks in a row, 
do item level comparisons between nodes (see below.)

Avatar's "database consistency hash": 
Generated by querying the DB only on each machine for each avatar. 
A hash is constructed from the sorted keys, 
adding the key, and the timestamp from the value, 
and the md5 from the stored value (if we have data) 
or a marker for a tombstone if we have one of those.

Item level comparisons: Pull all 10 databases. Iterate through them. 
(since they are all sorted, this doesn't require unbounded memory.) 
Ignore keys stored in the last hour, which may still be settling. 
Based on the timestamp values present for each key, 
you should be able to determine the "correct" state. 
I.e. if a tombstone is present, it means any earlier keys should not be there. 
If only some (but not all) shares are there, the remaining shares should be 
reconstructed and added. 
Any other situation would indicate a data integrity error 
that should be resolved.
"""
import logging
import os
import sys
import time
import uuid

from diyapi_tools import message_driven_process as process
from diyapi_tools.amqp_connection import local_exchange_name 
from diyapi_tools.low_traffic_thread import LowTrafficThread, \
        low_traffic_routing_tag

from messages.database_consistency_check import DatabaseConsistencyCheck
from messages.database_consistency_check_reply import \
    DatabaseConsistencyCheckReply

_log_path = u"/var/log/pandora/diyapi_anti_entropy_server_%s.log" % (
    os.environ["SPIDEROAK_MULTI_NODE_NAME"],
)
_queue_name = "anti-entropy-%s" % (
    os.environ["SPIDEROAK_MULTI_NODE_NAME"], 
)
_routing_header = "anti-entropy"
_routing_key_binding = ".".join([_routing_header, "*"])
_database_consistency_check_reply_routing_key = ".".join([
    _routing_header,
    DatabaseConsistencyCheckReply.routing_tag,
])
_low_traffic_routing_key = ".".join([
    _routing_header, 
    low_traffic_routing_tag,
])
_polling_interval = float(os.environ.get(
    "DIYAPI_ANTI_ENTROPY_POLLING_INTERVAL", "600.0")
)
_exchanges = os.environ["DIY_NODE_EXCHANGES"].split()
_error_hash = "*** error ***"

def _create_state():
    return dict()

def _next_poll_interval():
    return time.time() + _polling_interval

def _start_consistency_check(state, avatar_id):
    log = logging.getLogger("_start_consistency_check")
    log.info("start consistency check on %s" % (avatar_id, ))

    request_id = uuid.uuid1().hex
    timestamp = time.time()

    state[request_id] = dict()
    state[request_id]["avatar_id"] = avatar_id
    state[request_id]["timestamp"] = timestamp
    state[request_id]["replies"] = dict() 

    message = DatabaseConsistencyCheck(
        request_id,
        avatar_id,
        timestamp,
        local_exchange_name,
        _routing_header
    )
    # send the DatabaseConsistencyCheck to every node
    return [
        (dest_exchange, message.routing_key, message) \
        for dest_exchange in _exchanges
    ]

def _handle_low_traffic(_state, _message_body):
    log = logging.getLogger("_handle_low_traffic")
    log.debug("ignoring low traffic message")
    return None

def _handle_database_consistency_check_reply(state, message_body):
    log = logging.getLogger("_handle_database_consistency_check_reply")
    message = DatabaseConsistencyCheckReply.unmarshall(message_body)

    if not message.request_id in state:
        log.warn("Unknown request_id %s from %s" % (
            message.request_id, message.node_name
        ))
        return []

    request_id = message.request_id
    if message.error:
        log.error("%s (%s) %s from %s %s" % (
            state[request_id]["avatar_id"], 
            message.result,
            message.error_message,
            message.node_name,
            message.request_id
        ))
        hash_value = _error_hash
    else:
        hash_value = message.hash
        
    if message.node_name in state[request_id]["replies"]:
        log.error("duplicate reply from %s %s %s" % (
            message.node_name,
            state[request_id]["avatar_id"], 
            message.request_id
        ))
        return []

    state[request_id]["replies"][message.node_name] = hash_value

    if len(state[request_id]["replies"]) < len(_exchanges):
        return []

    # at this point we should have a reply from every node. Are they all the
    # same?
    hash_list = list(set(state[request_id]["replies"].values()))
    
    # ok - all have the same hash
    if len(hash_list) == 1 and hash_list[0] != _error_hash:
        log.info("avatar %s compares ok" % (state[request_id]["avatar_id"], ))
        del state[request_id]
        return []

    # we have error(s), but the non-errors compare ok
    if len(hash_list) == 2 and _error_hash in hash_list:
        error_count = 0
        for value in state[request_id]["replies"].values():
            if value == _error_hash:
                error_count += 1
        log.warn("avatar %s compares ok with %s nodes reporting errors" % (
            state[request_id]["avatar_id"], 
            error_count
        ))
        del state[request_id]
        return []

    # if we make it here, we have some form of mismatch, possibly mixed with
    # errors
    log.error("avatar %s hash mismatch" % (state[request_id]["avatar_id"], ))
    for node_name, value in state[request_id]["replies"].items():
        log.error("    node %s vlaue %s" % (node_name, value, ))
    del state[request_id]["avatar_id"]

    return []

_dispatch_table = {
    _database_consistency_check_reply_routing_key   : \
        _handle_database_consistency_check_reply,
    _low_traffic_routing_key            : _handle_low_traffic,
}

def _startup(halt_event, state):
    state["low_traffic_thread"] = LowTrafficThread(
        halt_event, 
        _routing_header
    )
    state["low_traffic_thread"].start()
    state["next_poll_interval"] = _next_poll_interval()

    return []

def _check_time(state):
    """check if enough time has elapsed"""
    log = logging.getLogger("_check_time")

    state["low_traffic_thread"].reset()

    if time.time() < state["next_poll_interval"]:
        return []

    state["next_poll_interval"] = _next_poll_interval()

    return []

def _shutdown(state):
    state["low_traffic_thread"].join()
    del state["low_traffic_thread"]
    return []

if __name__ == "__main__":
    state = _create_state()
    sys.exit(
        process.main(
            _log_path, 
            _queue_name, 
            _routing_key_binding, 
            _dispatch_table, 
            state,
            pre_loop_function=_startup,
            in_loop_function=_check_time,
            post_loop_function=_shutdown
        )
    )

