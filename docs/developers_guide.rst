Developer's Guide
=======================================================


.. toctree::
   :maxdepth: 10
   :numbered:

Overview
^^^^^^^^

TODO

Customers, Collections, Objects, and Permissions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A customer is someone using a Nimbus.io service.  Customers may create one or
more Collections to organize their data in.  A default collection for each
customer is created automatically.  Within a collection, objects maybe stored,
retrieved, listed, and removed.  Collections can optionally support Object
Versioning.

As of this writing, Nimbus.io only supports a the simplest permission system:
authenticated users can access their own objects, and nothing else.   This will
be expanded to allow objects to be set as publically readable.  That will be
followed by a more rich Unix style user and group permission model and ACLs.

Command line tools
^^^^^^^^^^^^^^^^^^

If you have `lumberyard` and `motoboto` (the Python libraries for accessing
Nimbus.io) installed, you can use a command line tool to list, delete, and
archive data into your Nimbus.io Collections.

Here are some examples using the `nio_cmd` command line tool::

    # list keys in collection 
    ls_nio collection_name 

    # delete a key in a collection
    nio_cmd rm collection_name key_name  

    # copy the local file filename.ext to key_name in collection_name 
    # (use a filename of - to copy stdin)
    nio_cmd cp filename.ext nimbus.io://collection_name/key_name  

    # copy the contents of key_name in collection_name mylocalfile.ext
    nio_cmd cp nimbus.io://collection_name/key_name mylocalfile.ext 

    # copy the contents of key_name in collection_name to the local file
    # mylocalfile.ext
    nio_cmd cp nimbus.io://collection_name/key_name mylocalfile.ext 

    # copy a key from one nimbus.io location to another
    nio_cmd cp nimbus.io://collection_name1/key_name1 nimbusio://collection_name2/key_name2 
    # copy a key from s3 to nimbus.io
    nio_cmd cp s3://bucket_name/key_name nimbusio://collection_name/key_name 

    # copy a key from nimbus.io to s3
    nio_cmd cp nimbusio://collection_name/key_name s3://bucket_name/key_name  

    # move a key from s3 to nimbus.io
    nio_cmd mv s3://bucket_name/key_name nimbusio://collection_name/key_name 

Running Nimbus.io locally for Dev
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Production Nimbus.io storage clusters operate across at least 10 independent
computers.  

For development and testing, you can simulate a full Nimbus.io cluster locally.
Included is a cluster simulator script which can spawn and configure all the
nodes for a full cluster on a single machine.  This is suitable for development
work or inclusion in unit tests.  

Clone the latest version of the source code::
    git clone https://nimbus.io/dev/git/nimbus.io/

Install all the needed libraries and other dependencies.  There are well
commented shell scripts to guide you through this for some operating systems in
the `scripts/install` folder within the source.  A very recent Linux
distribution may provide everything you need as packages.

In general, Nimbus.io depends on all of the following:

* Python 2.6.x or 2.7.x
* Python 3.2+
* PostgreSQL 9.0+ (9.1+ recommended)
* ZeroMQ 2.1.10+
* Python libraries: cython, gevent, gevent-zeromq, webob, zfec
* Optional: Perl StatGrabber library (for sending runtime stats to ganglia)
* Optional: Sphinx (for building the documentation, not required)

Once all of the dependencies are available, you can spawn a simulated test
cluster.  This will create everything needed to run Nimbus.io, simulating 10
nodes and a central database within a folder in your file system.  New
PostgreSQL instances will be initilized.  Configuration files for the overall
Storage Cluster and each of the 10 Storage Nodes will be written.  

The simulator will give you a command prompt where you can start and stop the
full cluster or specific nodes.

The script below will spawn a new simulated cluster inside a local folder.  You
will have 10 nodes, each listening on a local TCP port for REST API commands.
The first of these will be on whatever the `--baseport` argument was (default
`9000`).  If you get binding errors, try a different base port.  Read the
source for details.

::
    
    # by default, this will create the simulated cluster running in
    # /tmp/clustersim
    scripts/spawn_simulated_cluster.sh

You can use the command line tool for creating customer accounts within a local
cluster.  Note that this tool needs the ENV variables specififying the local
configuration to operate.  Here's a bash session loading the config scripts
(generated by the simulator above) and then using the tool.

::
    source /tmp/clustersim/config/central_config.sh
    source /tmp/clustersim/config/client_config.sh
    PYTHONPATH=$PWD python customer/customer_main.py --help
     

Run the standard unit tests against a simulated cluster (requires `lumberyard`
and `motoboto` libraries to be installed.).  The script will create a test user
`mutoboto-test-01` for the unit tests to use.

::

    scripts/run_tests_on_simcluster.sh /tmp/clustersim

There's a standard benchmarking suite that uses the `motoboto` library.  This
script creates many test users and invokes the tool with standand paramaters.
The tool itself is driven by a JSON config file that allows specifying a
variety of ways that the cluster will be used.

::
    scripts/run_greenlet_benchmark_on_simcluster.sh /tmp/clustersim
 
Accessing Nimbus.io via a Library
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You acn use Nimbus.io via the REST API directly over HTTP (described below.).
However the easiest way is to use a library that abstracts the REST  API and
provides a higher level interface.

The base client library for Nimbus.io is called `lumberyard` and is available
as a Python module.  

We've ported the popular Amazon S3 library `boto` to use Nimbus.io and we call
this module `motoboto`.  It internally uses `lumberyard`, but presents the same
interface as `boto` so applications using `boto` could use `motoboto` as an
easy alternative.

The development roadmap includes expanding the selection of client libraries to
include options for PHP, Perl, Ruby, Java, C#, Clojure, Node.js, Erlang, and
Haskel.  Please contact us if you'd like to create a library for your favorite
language, or even if you just have a recommendation on which popular S3
libraries you'd like to have ported to use Nimbus.io.

Similar to the Nimbus.io server software itself, all the client libraries we
write are free software released under the LGPL license.

API Usage
^^^^^^^^^
The basic idea of REST is that you have "resources" (URLs) that represent 
ideas or concepts in your problem domain, and then you use HTTP verbs to 
perform actions on the resources.

nimbus.io resources for a customer:

* The customer's account
* The collections owned by the customer
* The keys within a collection
* The data (and meta data) for a key

These are represented as::

   https://nimbus.io/customers/<username>
   https://nimbus.io/customers/<username>/collections
   https://<collection name>.nimbus.io/data/
   https://<collection name>.nimbus.io/data/<key>

Authentication
##############
Each request to the nimbus.io API must be authenticated using a key, 
assigned to you when you sign up for the service. You may request additional 
keys to be associated with your account if you wish.

Authentication is accomplished using the HTTP Authorization header with a 
special scheme name of NIMBUSIO. The format of this header is detailed below.

HTTP Authorization Header
+++++++++++++++++++++++++
The format of the HTTP Authorization header is:

Authorization: NIMBUS.IO <key_id>:<signature>

key_id
   An integer identifying the authentication key assigned to you by SpiderOak.


signature
   A hex string signature, generated for each request as described in
   Authentication Signature

Authentication Signature
++++++++++++++++++++++++
The authentication signature is a SHA256 HMAC hex string generated using a 
string representing the request and your authentication key. This string is 
made up of the following fields, separated by newline characters 
(ascii code 10):

username
   The username associated with the account. 


method
   The HTTP method being used for the request. At this time, 
   either GET, POST, or DELETE.


timestamp
   Integer number of seconds since the UNIX Epoch (1970-01-01 00:00:00 UTC).


uri
   the path part of the URI (minus the hostname) for example /data/my-key 

Important

The timestamp must agree within 10 minutes of that on the server, or the 
server will reject the authentication. Please synchronize your clock to an 
NTP server or otherwise make sure it is correct.

The following Python function will generate a valid signature for its inputs:

::

   import hmac, hashlib

   def make_signature(authenticaton_key, username, method, timestamp, uri_path):
       string_to_sign = '\n'.join((
           username,
           method,
           str(timestamp),
           uri_path
       ))
       hmac_value = hmac.new(authenticaton_key, string_to_sign, hashlib.sha256)
       return hmac_value.hexdigest()

X-NIMBUS-IO-Timestamp
+++++++++++++++++++++

To limit the useful lifetime of an authentication signature, the signature
includes a timestamp.  In order for the server to verify the signature, a
special header must be provided in the request with the same timestamp used to
generate the signature. The header is X-NIMBUS-IO-Timestamp, and the value is
the timestamp detailed in Authentication Signature. 

For example:

X-NIMBUS-IO-Timestamp: 1276808600

Authentication Example
++++++++++++++++++++++
Using the following example credentials, we use the make_signature function 
from Authentication Signature to generate an HTTP Authorization header.

::

   Username:	alice
   Authentication Key ID:	5001
   Authentication Key:	DwWKayqqnWnmLouZQKfncsNj72x7TThMA3uO9Y/IBJg
   Timestamp:	1276808600
   >>> make_signature('alice', 'GET', 1276808600, '/list_collections')
   'e0942c34ee095825302dd6aede9ac7f7c8fc5985998f9eaacbe906b673855876'

Resulting HTTP Headers:

::

   Authorization: NIMBUS.IO 5001:e0942c34ee095825302dd6aede9ac7f7c8fc5985998f9eaacbe906b673855876
   X-NIMBUS.IO-Timestamp: 1276808600

Customer's Account
##################

List Collections
++++++++++++++++
.. http:get:: /customer/<username>/collections

    List all collection names for this customer. The reply body will contain a JSON 
    list of pairs. Each pair will be collection name and date created.

    :statuscode 200: no error

Collections
###########

Creating a Collection
+++++++++++++++++++++
.. http:post:: /customer/<username>/collections

    Create a new collection. 

    If you try to create a collection with the same name as an existng collection,
    This query will NOT fail. It will reuse the existing collection. If the 
    existing collection has been deleted, it will be un-deleted.

    :query action: create 
    :query name: the name of the new collection 
    :statuscode 200: no error
 
Deleting a Collection
+++++++++++++++++++++
.. http:delete:: /customer/<username>/collections/<collection-name>
.. http:post:: /customer/<username>/collections/<collection-name>

    Delete an existing collection. [1]_ 

    :query action: delete (POST only) 

    :statuscode 200: no error
    :statuscode 403: forbidden to delete the default collection, or a collection contaning data.
    :statuscode 404: unknown collection

Getting Space Usage Information
+++++++++++++++++++++++++++++++
To get information on space usage by a collection 

.. http:get:: /customer/username/collections/<collection-name>

    Get usage information on the collection specified in the hostname 

    :query action: space_usage
    :statuscode 200: no error

Keys
####

Collections as Hostnames
++++++++++++++++++++++++

nimbus.io organizes the objects that you store into collections. Every 
nimbus.io key is a member of a collection. For efficient access to your 
data nimbus.io uses the collection name as part of the hostname_.

For example, to act on objects in the collection 'my-temperature-readings', 
your HTTP query would be directed to hostname 'my-temperature-readings.nimbus.io'

This approach requires some restrictions on your collection names:

* collection names must be unique: you cannot use a colection name that 
  someone else is already using.
* Internet standards mandate that collection names may contain only 
  the ASCII letters 'a' through 'z' (in a case-insensitive manner), 
  the digits '0' through '9', and the hyphen ('-').
* collection names must be between 1 and 63 characters long

nimbus.io gives you a default collection name of 'dd-<your user name>'

* you don't need to create your default collection
* you cannot delete your default collection

This paces the same restrictions on user name that apply to collection name, 
except that user name must be between 1 and 60 characters long.

To reduce the inconvenience of creating a unique collection name, nimus.io 
provides a facility for creating guaranteed unique names of the form
'rr-<your user-name>-<collection name>'. Of course, this must comply with the
restrictons mentioned above.

.. _hostname: http://en.wikipedia.org/wiki/Hostname

Object Keys
+++++++++++

Each object within a collection is uniquely identified by a key. The key must 
be between 1 and 1024 characters long. When used in an HTTP request, the key
must meet the standard HTTP restrictions. 

Nimbus.io does not impose or recognize any structure or hierarchy among the 
keys in a collection. 

You can organize your data hierarchically by using a delimiter character in 
your keys. The API doesn't place any restrictions on what you use as a 
delimiter, but may people use the slash character ("/") for as that is the 
standard delimiter for URLs. For example:

* https://dd-alice.nimbus.io/data/california/trafficjam.jpg
* https://dd-alice.nimbus.io/data/maui/sunset.jpg
* https://dd-alice.nimbus.io/data/maui/beach.jpg

You can list the keys you have stored in a collection by performing a 
listmatch query (See listing_keys_ for details). The nimbus.io API uses the 
JSON format for transferring information about your data, but the data itself 
is transferred as it is stored.

Uploading to a Key
++++++++++++++++++
To upload data to a collection in your account using the nimbus.io API, issue 
a POST request containing the key you wish the data to be stored at, with the 
data to be storeed as the body of the request.

The data will be uploaded to the collection specified in the HTTP hostname.

.. http:post:: /data/<key>

    Data to be uploaded comprises the body of the request.

    :query conjoined_identifier=<conjoined-identifier>: 
        * the value returned by conjoined start, 
        * for a non-conjoined key, set to "" or don't send at all
    :query conjoined_part=<sequence-number>: 
        * for conjoined archives, sequence number of this upload, starting at 1
        * for a non-conjoined key, set to zero or don't send at all
    :query __nimbus_io__<meta-key>=<meta-value>: metadata associated with the key 
    :statuscode 200: no error
    :statuscode 403: invalid data (probably zero size)
    :statuscode 404: unknown collection

.. _listing_keys:

Listing Keys
++++++++++++
To list the keys in a collection, issue a listmatch request. 

.. http:get:: /data/

    :query max_keys: The maximum number of keys to retrieve
    :query prefix: The prefix of the keys you want to retrieve
    :query marker: where you are in the result set
    :query delimiter: Keys that contain the same string between the prefix and the 
                first occurrence of the delimiter will be rolled up into a single 
                result element. 

    :statuscode 200: no error

Listing Versions
++++++++++++++++
To list the versions in a collection, issue a ?versions request. 

Server will return a JSON dictionary containing:

 * version_list (list of dictionaries)
 * truncated (boolean)

where the version_list entries contain:

 * key
 * version_identifier
 * timestamp 

.. http:get:: /?versions

    :query max_keys: The maximum number of versionss to retrieve
    :query prefix: The prefix of the keys you want to retrieve
    :query key_marker: where you are in the result set keys
    :query version_marker: where you are in the result set versions
    :query delimiter: Keys that contain the same string between the prefix and the 
                first occurrence of the delimiter will be rolled up into a single 
                result element. 

    :statuscode 200: no error

Downloading From a Key
++++++++++++++++++++++
Downloading an object from a collection is just a simple GET request. 
The server will return exactly what you uploaded to that URL:

.. http:get:: /data/<key>

    :statuscode 200: no error

Deleting a key
++++++++++++++
To delete a resource, 
* issue a DELETE request
* issue a POST request for delete [1]_

.. http:delete:: /data/<key>


.. http:post:: /data/<key>

    :query action=delete: delete action for post request
    :statuscode 200: no error

Getting File Information About a Key
++++++++++++++++++++++++++++++++++++
To retrieve file information about a key issue a HEAD request.

* file_size
* md5 digest of file contents

.. http:head:: /data/<key>

    :statuscode 200: no error
    :statuscode 404: not found

Getting Meta Information About a Key
++++++++++++++++++++++++++++++++++++
To retrieve the meta data stored with a key issue a 'meta' request.

body will be a JSON list of key/value pairs

.. http:get:: /data/<key>/

    :query action=meta: request meta
    :statuscode 200: no error
    :statuscode 404: not found

Conjoined Archives:
###################
A conjoined archive enables multiple parts of a file to be uploaded in 
parallel. And/or for individual uploads to be restarted without restarting
the entire archive.

**Note**: As of this writing, support for Conjoined Archives is incomplete.

Listing Conjoined Archives
++++++++++++++++++++++++++
List the conjoined archives active for this collection 

Server will return a JSON dictionary containing:

 * conjoined_list (list of dictionaries)
 * truncated (boolean)

where the conjoined_list entries contain:

 * conjoined_identifier 
 * key
 * create_timestamp 
 * abort_timestamp
 * complete_timestamp 
 * delete_timestamp

.. http:get:: /conjoined/

    :query max_conjoined: 
        The maximum number of conjoined archives to retrieve. 
        Default value is 1000.

    :query key_marker:
        list keys greater than this key

    :query conjoined_identifier_marker:
        If key_marker is specified, only list keys with conjoined_identifier
        greater than this identifier.
    :statuscode 200: no error

Start a Conjoined Archive
+++++++++++++++++++++++++
Start a conjoined archive, where multiple uploads can combine to form the key
The body of the return contains a JSON conjoined_identified (UUID)

.. http:post:: /conjoined/<key>

    :query action=start: start a conjoined archive
    :statuscode 200: no error

Upload to a Conjoined Archive
+++++++++++++++++++++++++++++
Upload one part of the full key. Multiple uploads can be done in parallel.
This is a normal key upload with additional varaibles.

.. http:post:: /data/<key>

    :query conjoined_identifier=<conjoined-identifier>: returned by start
    :query conjoined_part=<sequence-number>: number of this upload
    :statuscode 200: no error

Finish Conjoined Archive
++++++++++++++++++++++++
Mark the archive as completed. 

.. http:post:: /conjoined/<key>

    :query action=finish: finish a conjoined archive
    :query conjoined_identifier=<conjoined-identifier>: returned by start
    :statuscode 200: no error

Abort Conjoined Archive
++++++++++++++++++++++++
Halt the conjoined archive and release all resources.

.. http:delete:: /conjoined/<key>

    :query action=abort: abort a conjoined archive
    :query conjoined_identifier=<conjoined-identifier>: returned by start
    :statuscode 200: no error

List Uploads of Conjoined Archive
+++++++++++++++++++++++++++++++++
List the known uploads in sequence

.. http:get:: /conjoined/<key>/<conjoined-identifier>/

    :statuscode 200: no error

.. [1] In an ideal world, we would just need DELETE for the this. But due to
   limited browser support for the DELETE verb, we also provide an alternative
   via POST with action=delete. 


Migrating data
^^^^^^^^^^^^^^

Nimbus.io is intended to facilitate easy bidirectional migration between
Nimbus.io, Amazon S3, and other cloud storage systems.  

Since the Nimbus.io platform is available under the AGPL free software license,
there's also always the option to bring your data home.

TODO to further ease interoperability, the development roadmap includes remote
copy additions to the API, so that data can be directly transferred between the
Nimbus.io storage service, S3, and other network accessible resources.

